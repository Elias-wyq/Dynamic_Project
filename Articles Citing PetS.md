## Articles Citing PetS

the one with **bold font** is developed based on PetS

- Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey [link to paper](https://arxiv.org/abs/2403.14608)

- PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU [link to paper](https://arxiv.org/abs/2312.12456)

- AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving [link to paper](https://www.usenix.org/conference/osdi23/presentation/li-zhouhan)
- Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems [link to paper](https://arxiv.org/abs/2312.15234)
- dLoRA: Dynamically Orchestrating Requests and Adapters for LoRA LLM Serving [link to paper](https://www.usenix.org/conference/osdi24/presentation/wu-bingyang)

- DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency [link to paper](https://arxiv.org/abs/2408.00741)

- DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving [link to paper](https://arxiv.org/abs/2401.09670)

- CA-LoRA: Adapting Existing LoRA for Compressed LLMs to Enable Efficient Multi-Tasking on Personal Devices [link to paper](https://arxiv.org/abs/2307.07705)

- DeltaZip: Efficient Serving of Multiple Full-Model-Tuned LLMs [link to paper](https://arxiv.org/abs/2312.05215)

- Edge-LLM: A Collaborative Framework for Large Language Model Serving in Edge Computing [link to paper](https://ieeexplore.ieee.org/abstract/document/10707514) 

- CARASERVE: CPU-Assisted and Rank-Aware LoRA Serving for Generative LLM Inference [link to paper](https://arxiv.org/abs/2401.11240)

- **OTAS: An Elastic Transformer Serving System via Token Adaptation** [link to paper](https://arxiv.org/abs/2401.05031)

- Parameter-Efficient Fine-Tuning with Discrete Fourier Transform [link to paper](https://arxiv.org/abs/2405.03003)

- FastPTM: Fast weights loading of pre-trained models for parallel inference service provisioning [link to paper](https://www.sciencedirect.com/science/article/pii/S0167819124000528)

- Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments [link to paper](https://arxiv.org/abs/2411.17741)

- ITIF: Integrated Transformers Inference Framework for Multiple Tenants on GPU [link to paper](https://dl.acm.org/doi/abs/10.1145/3605573.3605585)

- Liger: Interleaving Intra- and Inter-Operator Parallelism for Distributed Large Model Inference [link to paper](https://dl.acm.org/doi/abs/10.1145/3627535.3638466)

- Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters [link to paper](https://openreview.net/forum?id=HfpV6u0kbX)

- DYNAMIC LORA SERVING SYSTEM FOR OFFLINE CONTEXT LEARNING [link to paper](https://people.eecs.berkeley.edu/~kubitron/courses/cs262a-F23/projects/reports/project1011_paper_92116151989678177816.pdf)

## Basic Work

- BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models .[link to paper](https://arxiv.org/abs/2404.18322)
